{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelacion de los datos de Walmart\n",
    "\n",
    "A continuación se describe el enfoque seguido para clasificar los tipos de las visitas de los clientes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos los paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid\n",
    "from sklearn import preprocessing, svm, metrics, tree, decomposition\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from time import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directorios de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios donde se ubican los datos, asi como donde se guardaran los modelos y sus resultados\n",
    "temp = os.path.expanduser('../../data')\n",
    "results = os.path.expanduser('/results')\n",
    "models_dir = os.path.expanduser('/results/models')\n",
    "submission = os.path.expanduser('/results/submissions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leemos los datos para trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de los datos de entrenamiento y prueba\n",
    "addr= os.path.join(temp, \"walmart_test_trip_dummies.csv\")\n",
    "df_test = pd.read_csv(addr, index_col=0)\n",
    "\n",
    "df_test.sample(frac=0.1, replace=True, random_state=1)\n",
    "\n",
    "addr= os.path.join(temp, \"walmart_train_trip_dummies.csv\")\n",
    "df_train = pd.read_csv(addr, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion auxiliar para crear el archivo que se subira a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submit(y_pred, nom):\n",
    "    test_predictions = pd.DataFrame(y_pred)\n",
    "    test_indexes = df_test.visitnumber\n",
    "    test_predictions.insert(0, 'VisitNumber', test_indexes)\n",
    "    test_predictions.columns = ['VisitNumber', 'TripType_3','TripType_4','TripType_5','TripType_6','TripType_7',\\\n",
    "    'TripType_8','TripType_9','TripType_12','TripType_14','TripType_15','TripType_18',\\\n",
    "    'TripType_19','TripType_20','TripType_21','TripType_22','TripType_23','TripType_24',\\\n",
    "    'TripType_25','TripType_26','TripType_27','TripType_28','TripType_29','TripType_30',\\\n",
    "    'TripType_31','TripType_32','TripType_33','TripType_34','TripType_35','TripType_36',\\\n",
    "    'TripType_37','TripType_38','TripType_39','TripType_40','TripType_41','TripType_42',\\\n",
    "    'TripType_43','TripType_44','TripType_999']\n",
    "\n",
    "    name = \"sub\" + str(nom) + \".csv\"\n",
    "    addr= os.path.join(submission, name)\n",
    "    test_predictions.to_csv(addr, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para realizar el gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_hyper_params():\n",
    "    clfs = {\n",
    "        'RF': RandomForestClassifier(n_estimators=10),\n",
    "        'ET': ExtraTreesClassifier(n_estimators=10, criterion='entropy'),\n",
    "        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=200),\n",
    "        'LR': LogisticRegression(penalty='l2', C=1e5, solver='saga',max_iter=2),\n",
    "        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),\n",
    "        'NB': GaussianNB(),\n",
    "        'DT': DecisionTreeClassifier(),\n",
    "        'SGD': SGDClassifier(loss=\"hinge\", penalty=\"l2\"),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=1),\n",
    "        'XGB': XGBClassifier(objective= 'multi:softprob', num_class=38,\n",
    "                eval_metric= 'mlogloss', max_delta_step= 5)\n",
    "            }\n",
    "\n",
    "    grid = {\n",
    "        'RF':{'Classifier__n_estimators': [200,1000,2000], 'Classifier__max_depth': [20,50,100],\n",
    "              'Classifier__max_features': ['sqrt','log2'],'Classifier__min_samples_split': [2,5,10]},\n",
    "        #'LR': { 'Classifier__penalty': ['l2','elasticnet'], 'Classifier__C': [20,50,100], 'Classifier__l1_ratio': [0.3, 0.7, 0.8,0.9]},\n",
    "        #'LR': { 'Classifier__penalty': ['elasticnet'], 'Classifier__C': [20], 'Classifier__l1_ratio': [0.3]},\n",
    "        'LR':{},\n",
    "        'SGD': { 'Classifier__loss': ['hinge','log','perceptron'], 'Classifier__penalty': ['l2','l1','elasticnet']},\n",
    "        'ET': { 'Classifier__n_estimators': [1,10,100,1000,10000], 'Classifier__criterion' : ['gini', 'entropy'] ,\n",
    "               'Classifier__max_depth': [1,5,10,20,50,100], 'Classifier__max_features': ['sqrt','log2'],\n",
    "               'Classifier__min_samples_split': [2,5,10]},\n",
    "        'AB': { 'Classifier__algorithm': ['SAMME', 'SAMME.R'], 'Classifier__n_estimators': [1,10,100,1000,10000]},\n",
    "        'GB': {'Classifier__n_estimators': [1,10,100,1000,10000], 'Classifier__learning_rate' : [0.001,0.01,0.05,0.1,0.5],\n",
    "               'Classifier__subsample' : [0.1,0.5,1.0], 'Classifier__max_depth': [1,3,5,10,20,50,100]},\n",
    "        'NB' : {},\n",
    "        'DT': {'Classifier__criterion': ['gini', 'entropy'], 'Classifier__max_depth': [1,5,10,20,50,100],\n",
    "               'Classifier__max_features': ['sqrt','log2'],'Classifier__min_samples_split': [2,5,10]},\n",
    "        'SVM' :{'Classifier__C' :[0.00001,0.0001,0.001,0.01,0.1,1,10],'Classifier__kernel':['linear', 'rbf']},\n",
    "        'KNN' :{'Classifier__n_neighbors': [1,5,10,25,50,100],'Classifier__weights': ['uniform','distance'],\n",
    "                'Classifier__algorithm': ['auto','ball_tree','kd_tree']},\n",
    "        'XGB':  {'Classifier__learning_rate': [0.2, 0.3, 0.5], 'Classifier__max_depth': [6, 9, 12],\n",
    "                   'Classifier__subsample': [0.8, 1.0],  'Classifier__colsample_bytree': [0.8, 1.0]}\n",
    "\n",
    "        }\n",
    "\n",
    "    return clfs, grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definimos una función auxiliar para realizar el magic loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_loop(models_to_run, clfs, grid, pipeline, df_train, df_test, search =1):\n",
    "    type_labs = list(set(df_train.triptype.ravel()))\n",
    "    y = df_train.triptype.ravel()\n",
    "    X = df_train.drop(['triptype'], axis=1)\n",
    "\n",
    "\n",
    "    for index, clf in enumerate([clfs[x] for x in models_to_run]):\n",
    "        model_name = models_to_run[index]\n",
    "        print(model_name)\n",
    "        parameter_values = grid[models_to_run[index]]\n",
    "        #parameter_values['PCA__n_components'] = [4,6,8,77] # [2,4,6,78]\n",
    "        try:\n",
    "            pipeline.set_params(Classifier = clf)\n",
    "            if(search):\n",
    "                gs = GridSearchCV(pipeline, parameter_values, cv=StratifiedKFold(2),\n",
    "                                  n_jobs=-1, scoring='neg_log_loss', verbose= 1)\n",
    "            else:\n",
    "                gs = RandomizedSearchCV(pipeline, parameter_values, cv=StratifiedKFold(2),\n",
    "                                         n_jobs=-1, scorer=sklearn.metrics.log_loss(labels=type_labs))\n",
    "            start = time()\n",
    "\n",
    "            gs.fit(X, y)\n",
    "\n",
    "            print(\"GridSearch time: \" + (str)(time() - start))\n",
    "            \n",
    "            print(\"Fin del magic_loop\")\n",
    "\n",
    "            ## Guardar el model en un archivo\n",
    "            #nom_pkl = \"model_\" + str(model_name) +\"_dic19.pkl\"\n",
    "            #addr= os.path.join(models_dir,nom_pkl)\n",
    "            #with open(addr, \"wb\") as f:\n",
    "            #     pickle.dump(gs, f)\n",
    "            \n",
    "            #nom_csv = \"cv_\"+ str(model_name) +\"_dic19.csv\"\n",
    "            #addr= os.path.join(results, nom_csv)\n",
    "            #pd.DataFrame(gs.cv_results_).to_csv(addr)\n",
    "        \n",
    "            #y_pred = gs.predict_proba(df_test)\n",
    "            #nom_csv = \"_\" + str(model_name) +\"_dic19\"\n",
    "            #create_submit(y_pred, nom_csv) \n",
    "\n",
    "\n",
    "        except IndexError as e:\n",
    "            print('Error:', e)\n",
    "            continue\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definicion del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('VarianceThreshold', VarianceThreshold()),\n",
    "    ('Normalizer', StandardScaler()),\n",
    "    ('Classifier', RandomForestClassifier(n_estimators=10, n_jobs=-1))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos a probar en el magic loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   37.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   37.0s finished\n",
      "/Users/czammar/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/czammar/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch time: 132.17105412483215\n",
      "Fin del magic_loop\n"
     ]
    }
   ],
   "source": [
    "clfs, grid = define_hyper_params()\n",
    "results_S = magic_loop( models, clfs, grid, pipeline, df_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_S"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
