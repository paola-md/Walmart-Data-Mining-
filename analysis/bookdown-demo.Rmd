--- 
title: "Proyecto Walmart"
author: "Beto, Cesar, Luis y Paola"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
code_folding: "hide"
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "El objetivo de libro es describir el proceso de CRISP-DM."
---

```{r eval=FALSE}
#install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
library("bookdown")
```

```{r include=FALSE}
instalar <- function(paquete) {
  if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
    install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
    library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
  }
}

paquetes <- c("odbc","readr", "stringr", "dplyr", "tidyverse", "bookdown",
              "funModeling","magrittr")

lapply(paquetes, instalar)

# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
  
), 'packages.bib')

```


# Breve descripción del proyecto 

El presente trabajo busca explorar un conjunto de datos de los clientes de Walmart, a través de técnicas de análisis de datos y aprendizaje de máquina, con el objeto de agrupar los tipos de visitas que hacen los clientes en torno a una serie de categorías desarrollas por la empresa de manera interna, empleando la metodología CRIPS-DM.

<!--chapter:end:index.Rmd-->

# Comprensión del negocio

## Antecedentes

Uno de los mayores intereses de las cadenas comerciales es conocer el comportamiento de sus clientes con el objeto de poder implementar estrategias de venta acordes a los diferentes necesidades existentes en el público. 

En este sentido, un enfoque que puede ser de utilidad es segmentar las visitas que los clientes efectúan en los establecimientos en diferentes tipos de viajes; ello significa determinar un conjunto categorías que reflejen los motivos que se encuentran detrás de una visita a un establecimiento. Para tal efecto, se puede echar mano de la información histórica que se tenga sobre el cliente, en términos de 1) los datos personales de este (por ejemplo, su información socio-económica) y 2) las transacciones de artículos que este haya realizado (tanto para adquirir artículos como para devolverlos por algún motivo); de manera que sea posible delinear un patrón de comportamiento que permita extraer elementos de valor para personalizar experiencias de comprar acordes a la realidad del mercado.

A manera de ejemplo, se puede pensar que existen diferencias entre los clientes que hacen una visita rápida a un establecimiento para comprar dulces, que aquellos que surten sus alacenas para consumo de víveres en la de semana.

Como parte de este interés, Walmart, la cual es una cadena comercial con amplia presencia alrededor del mundo en la venta de artículos de tecnología, hogar, linea blanca, supermercado y muchos otros, ha desarrollado una metodología a nivel interno que le permite agrupar en torno a 38 categorías a las diferentes visitas que realizan sus clientes, en función de los artículos que los clientes adquieren.

En tal contexto, durante 2015, esta cadena hizo disponible, a través de Kaggle, un conjunto de datos de las visitas de sus clientes[^1], que reflejan dicha categorización de los visitas de sus clientes, con el objeto de incentivar a científicos de datos a recrearla y explorar si ésta puede refinarse, redundando en un proceso de mejora en la segmentación de su clientela. 

Es así que la idea de este proyecto es analizar los datos aportados por Walmart para proponer un enfoque que permita recrear la categorización hecha por esta empresa, basándose en métodos de aprendizaje de máquina a través de la metodología CRIPS-DM, mediante los conceptos vistos en el curso de Minería y Análisis de Datos.

[^1]: Véase https://www.kaggle.com/c/walmart-recruiting-trip-type-classification/overview


## Determinación del objetivo

El objetivo de este proyecto proponer una metodología para resolver el problema de clasificación de los clientes de Walmart, presentes en las bases de datos que dicha empresa compartió a Kaggle en el año 2015, a partir del análisis de datos y métodos de aprendizaje de máquina.
 
## Determinación de criterio de éxito

Como criterio para determinar el cumplimiento del objetivo del proyecto, se estableció que el modelo propuesto tenga un mejor desempeño que el bechmark de la referida competencia de Kaggle. 

## Plan del proyecto

En línea con la exposición previa, a continuación se presenta el plan de proyecto para lograr el objetivo de este proyecto, mismo que se llevará a cabo a través de las fases que se describen, en alto nivel, a continuación:

* **Comprensión de los datos de Walmart:** conformada por las etapas de 1) extracción de la información aportada por Walmart, en su estado puro (es decir, datos crudos, sin realizar ningún tratamiento de la información); y 2) el estudio de las variables que componen el conjunto de datos de Walmart, junto con un proceso de exploración de la información contenida en ellos, en términos de una variable, pares de variables o múltiples combinaciones de ellas.
* **Preparación de los datos:** consistente en el proceso de selección e integración de los datos que serán útiles para plantear un metodología de clasificación de los clientes de Walmart, así limpieza de los datos (consideran la imputación en el caso de variables con información no disponible o ausente por algún motivo), y la ingeniería de características pertinente para mejorar el desempeño del enfoque propuesto.
* **Modelado:** corresponde al diseño de un conjunto de modelos, basados en aprendizaje de máquina, encaminados a resolver el problema de clasificación de los viajes de los clientes de Walmart, así como los criterios considerados para seleccionar el modelo con el mejor desempeño y el ajuste realizado para calibrar sus hiper-parámetros.
* **Evaluación:** se refiere a la etapa en donde se realiza la evaluación del modelo óptimo seleccionado como óptimo en la etapa previa, su posterior re-entrenamiento tras conjuntar los datos de entrenamiento y prueba con hiper-parámetros optimizados así como el reporte de la posición final en el tablero de Kaggle del desempeño logrado.
* **Implantación:** relativo al desarrollo de web service en flask para predecir resultados con el modelo final a partir de nuevos datos, así como el reporte ejecutivo que relata los principales hallazgos e hitos del proyecto.

Para mejor referencia, se provee un repositorio en Github https://github.com/paola-md/Walmart-Data-Mining- que conjunta los archivos de trabajo realizados con motivo del proyecto en cuestión, particularmente la totalidad de scripts en Bash, R y Python, así como las instrucciones a través de las cuales se podrá replicar el contenido del proyecto.

<!--chapter:end:01-comprension-negocio.Rmd-->

# Comprensión de los datos 

## Recolección de datos

De acuerdo a la documentación disponible[^1], uno de los requisitos necesarios para descargar los datos de visitas de los clientes de Walmart, es aportar la credenciales de un usuario registrado en el Kaggle.

En este caso, para facilitar este proceso, se implementó un programa en Bash, denominado download_extract_data.sh, el cual aprovecha la herramienta Wget de UNIX, para realizar la descarga de los datos del sitio electrónico en comento, recibiendo un archivo de configuración con las credenciales de autenticación de un cierto usuario (archivo cookies.txt).

A su vez, se creó un script en R (wrinting_feather.R, presente en la carpeta /data) que convierte los archivos .csv a formato .feather.

Como resultado, dentro de la carpeta /data se obtiene los siguientes archivo en formato .csv y .feather:

* **test.csv:** el cual contiene los datos del conjunto de entrenamiento,
* **train.csv:** relativo a los datos de entrenamiento, y finalmente,
* **sample_submission.csv:** es un ejemplo del formato en que se deben aportar los datos al sistema de Kaggle para la evaluación del desempeño del modelo de clasificación propuesto para el problema que nos ocupa.
* **test.feather**,
* **train.feather**.

[^1]: Véase https://github.com/Kaggle/kaggle-api

** Instrucciones para descarga de datos**

1. Debemos darle permisos de ejecución al script de Bash que se encuentra en la carpeta */build/comprension_datos* desde la terminal:

```
chmod +x download_extract_data.sh
```
2. Posteriormente ejecutamos dicho programa:

```
./download_extract_data.sh
```

3. Como se ha mencionado, el resultado de la ejecución de este script es la descarga de tres archivos dentro de la carpeta /data (train.csv, test.csv y sample_submission.csv).

## Análisis exploratorio de datos

De acuerdo a la documentación de Kaggle, la información de Walmart se proporciona en términos de las siguientes 7 variables:
  
* **TripType:** Variable objetivo. Son 38 diferentes categorias,
* **Visit Number:**  código identificardor de la visita de un usuario al establecimiento (o simplemente, "viaje"),
* **Weekday:** Día de la semana en que ocurrió el viaje del cliente,
* **UPC** corresponde al número de barras  de cada producto (es decir, un código identidicador del mismo),
* **ScanCount:** relativo al número de productos involucrados en la transacción del cliente. Cabe descatar que si se trada de una devolución, se representa como un número negativo
* **Department Description:** corresponde a una descripción de la categoria a la que pertenece el producto involucrado en la transacción del cliente.
* **Fineline Number:** relativo a un código identificador de 5,196 productos.


En la carperta de comprensión de datos (*build/comprension_datos*) dentro de la carpeta build se encuentra un shiny app que permite al usuario explorar visualmente la base de datos en un formato *tidy*. A continuación, se presentan los hallazgos más importantes.

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(knitr)
library(plotly)
library(feather)
library(kableExtra)
library(janitor)
library(tidygraph)
library(visNetwork)
library(ggraph)
library(purrr)
library(scales)
knitr::opts_chunk$set(echo = TRUE)
```

## Carga de los datos

```{R}
train <- read_feather('datos/train.feather')
```

## Funciones auxiliares


```{R}
cuantos <- function(x){
length(unique(x))
}
```

```{R}
# Funciones para crear una tabla con distribuciones porcentales de los campos en una columna
distrib <- function(x){
  tablita <- table(x)
  df <- cbind(tablita,round(prop.table(tablita)*100,4))
  colnames(df) <- c('Frecuencia','Porcentaje')
  df
}

distrib_val <- function(x,n){
  do.call(rbind,lapply(x[n],distrib))
}
```

## Análisis Univariado

A continuación comenzamos nuestro análisis exploratorio de datos viendo la distribución de las categorías presentes dentro de la base de datos y visualizando algunas variables importantes.


```{R}
summary(train)
```



## Upc
Tenemos bastantes productos distintos a escoger entre las canastas. Esto lleva a conjeturar que la base de datos se levantó a partir de varias tiendas Walmart.
```{R}
cuantos(train$Upc)
```

```{R}
dt <- train %>% count(Upc) %>%  mutate(prop = round(prop.table(n)*100,2))%>% arrange(desc(prop)) %>% head()
kable(dt) %>%  kable_styling(bootstrap_options = "striped", full_width = F) 
```

## ScanCount
Las cantidades de productos comprados en las canastas es bastante pequeña, tanto que hasta podría considerarse una categoría. Sin embargo, esto no se hizo.
```{R}
cuantos(train$ScanCount)
```

```{R}
dt <- train %>% count(ScanCount) %>%  mutate(prop = round(prop.table(n)*100,4))%>% arrange(desc(prop)) 
kable(dt) %>%  kable_styling(bootstrap_options = "striped", full_width = F) 
```

## FilelineNumber
Esta es una descripción interna hecha por Walmart, que para nuestros fines, mete más ruido de lo que nos interesa.
```{R}
cuantos(train$FinelineNumber)
```

```{R}
dt <- train %>% count(FinelineNumber) %>%  mutate(prop = round(prop.table(n)*100,2))%>% arrange(desc(prop))
kable(dt) %>%  kable_styling(bootstrap_options = "striped", full_width = F) 
```

## DepartmentDescription
El departamento al final es de las variables más importantes. En la sección de ingeniería de características se crearon nuevas variables que representában la probabilidad de que, dado los productos que se compraron en una canasta, esa canasta pertenezca a un departamento dado.
```{R}
cuantos(train$DepartmentDescription)
```

```{R}
dt <- train %>% count(DepartmentDescription) %>%  mutate(prop = round(prop.table(n)*100,2))%>% arrange(desc(prop)) 
kable(dt) %>%  kable_styling(bootstrap_options = "striped", full_width = F) 
```

Regresaremos a esta variable en el EDA multivariado.

## Variable respuesta

Nuestra variable de respuesta consta de 38 categorías.

```{R}
cuantos(train$TripType)
```


```{R}
dt <- train %>% count(TripType) %>%  mutate(prop = round(prop.table(n)*100,2))%>% arrange(desc(prop)) 
kable(dt) %>%  kable_styling(bootstrap_options = "striped", full_width = F) 
```



```{r}
gr <- train %>% 
  clean_names() %>% 
    select(trip_type) %>% 
    purrr::set_names(nm = c('x')) %>%
   mutate(id = 1:n()) %>% 
    group_by(x) %>%
    tally() %>% 
   rename( count = n)
 
 gr%>%
    plot_ly(labels = ~x, values = ~count) %>%
    add_pie(hole = 0.6) %>%
    layout(title = paste("Composición de la variable tripType ,respecto a las canastas (variable upc)"),  
                         showlegend = F,
           xaxis = list(showgrid = FALSE, zeroline = FALSE, 
                        showticklabels = FALSE),
           yaxis = list(showgrid = FALSE, zeroline = FALSE, 
                        showticklabels = FALSE)) %>%
   layout(legend = list(orientation = 'h'))

```

Nótese que la base de datos está treméndamente desbalanceada, pues tan sólo 4 de las categorías (37,38,39 y 40) acumulan más del 50% de los productos comprados en cada canasta.

## Visitnumber
Ahora, haciendo una agrupación hacia dentro de los tickets de las visitas, vemos que la variable de respuesta está más balanceada.

```{R}
cuantos(train$VisitNumber)
```


```{R}
dt <- train %>% count(VisitNumber) %>%  mutate(prop = round(prop.table(n)*100,4))%>% arrange(desc(prop)) %>% head()
kable(dt) %>%  kable_styling(bootstrap_options = "striped", full_width = F) 
```


```{r}
gr <- train %>% 
    select(TripType, VisitNumber) %>% 
    unique() %>% 
    select(TripType) %>% 
    purrr::set_names(nm = c('x')) %>%
   mutate(id = 1:n()) %>% 
    group_by(x) %>%
    tally() %>% 
   rename( count = n)
 
 gr%>%
    plot_ly(labels = ~x, values = ~count) %>%
    add_pie(hole = 0.6) %>%
    layout(title = paste("Composición de la variable tripType ,respecto a las canastas (variable visitNumber)"),  
                         showlegend = F,
           xaxis = list(showgrid = FALSE, zeroline = FALSE, 
                        showticklabels = FALSE),
           yaxis = list(showgrid = FALSE, zeroline = FALSE, 
                        showticklabels = FALSE)) %>%
   layout(legend = list(orientation = 'h'))
```

 Sin embargo, las categorías mencionadas con anterioridad dejan de ser las dominantes, con excepción de la 40 y la 39. Ahora, entran al juego la 8, la 9 y la 999.

## Weekday

Primero, nótese que la grán mayoría de las canastas se compraron en fin de semana (>50% de los tickets ocurrieron de viernes a domingo).

```{R}
cuantos(train$Weekday)
```

```{R}
dt <- train %>%select(VisitNumber, Weekday) %>% unique %>%  count(Weekday) %>%  mutate(prop = round(prop.table(n)*100,2))%>% arrange(desc(prop)) 
kable(dt) %>%  kable_styling(bootstrap_options = "striped", full_width = F) 
```

## Análisis Bivariado

En la sección anterior se notó que casi todas las canastas ocurrieron en fin de semana. Veamos cómo se distribuyen respecto a la variable de respuesta tanto respecto a productos brutos como respecto a canastas.

Ahora, respecto a productos brutos, filtraremos para las categorías que acaparan la mayor cantidad de datos en la base (37,38,39 y 40).

```{r}
ggplotly(  train %>% 
    select(TripType, Weekday) %>% 
    filter(TripType %in% 37:40) %>% 
    mutate(TripType = factor(TripType)) %>% 
    purrr::set_names(nm = c('x','y')) %>%
    group_by(x,y) %>% 
    tally %>% 
    ungroup %>% 
    group_by(x) %>% 
    mutate(porc = 100*n/sum(n)) %>% 
    ungroup %>% 
    ggplot(aes(x = x, y = porc))+
    geom_col(aes(fill = y), width = 0.7)+
    theme_minimal()+
    labs(title = 'Composición de variables categóricas de acuerdo a los productos',
         x = 'TripType',
         y = 'Porcentaje',
         fill = 'Weekday')+
    theme(axis.text.x = element_text(angle = 90))) %>% 
  layout(legend = list(orientation = 'h'))


```

Vemos que, respecto a la cantidad de productos, la distribución de los tipos de canasta es bastante uniforme a través de los días de la semana.


Ahora exploremos el número tickets de los tipos de canastas la distribución de los días de la semana, ahora filtrando para los tipos de canasta que resultaron como categorías dominantes en los conteos.

```{r}
ggplotly(  train %>% 
    select(TripType, Weekday, VisitNumber) %>% 
    filter(TripType %in% c(40,39,8,9,999)) %>% 
      unique %>% 
      select(-VisitNumber) %>% 
    mutate(TripType = factor(TripType)) %>% 
    purrr::set_names(nm = c('x','y')) %>%
    group_by(x,y) %>% 
    tally %>% 
    ungroup %>% 
    group_by(x) %>% 
    mutate(porc = 100*n/sum(n)) %>% 
    ungroup %>% 
    ggplot(aes(x = x, y = porc))+
    geom_col(aes(fill = y), width = 0.7)+
    theme_minimal()+
    labs(title = 'Composición de variables categóricas de acuerdo a los tickets',
         x = 'TripType',
         y = 'Porcentaje',
         fill = 'Weekday')+
    theme(axis.text.x = element_text(angle = 90))) %>% 
  layout(legend = list(orientation = 'h'))
```


De nuevo vemos que los días de la semana resultan balanceados en su distribución a través de los TripType.

## Análisis Multivariado

Ahora, con base en los hallazgos anteriores, se decidió crear la siguiente base de datos con técnicas de ingeniería de características que se explican en la sección de preparación de datos. Aquí presentamos los hallazgos más importantes de estás nuevas variables donde se muestran, dado una canasta en particular (un ticket), la probabilidad que pertenezcan los productos dentro de ella a un departamento dado.

```{r}
walmart_train_trip <- readRDS('datos/walmart_train_trip.RDS')

kable(head(walmart_train_trip)) %>%  kable_styling(bootstrap_options = "striped", full_width = F) 
```


Ahora, hay que entender mejor la relación entre los tipos de departamento y los tipos de canastas de los tickets en la base de datos., Entonces, en la base datos transformada se tienen los tickets y la probabilidad de que un departamento sea el dominante (en el sentido de que la mayoría de los productos de ese ticket pertenezca sólo a un departamente, una frecuencia hacia dentro del ticket), así al tomar el valor esperado respecto al tipo de canasta (*triptype*) de todas estas probabilidades. Ahora, buscamos encontrar los departamentos cuyas probabilidades se muevan en el mismo sentido de acuerdo al tipo de canasta. Así pues, conseguimos el siguiente mapa de calor: 


```{r}
tt <- walmart_train_trip %>% 
  select(contains('prob'), triptype) %>% 
  group_by(triptype) %>% 
  # summarise(val = sum(producto_cantidad,na.rm =T)) %>%
  summarise_all(mean) %>% 
  ungroup 

correlation <- tt %>%
  .[,-1]%>%  
  cor() %>% 
  as.data.frame() %>% 
  rownames_to_column(var = 'var1') %>% 
  gather(var2, val, -var1) %>% 
  unique %>% 
  filter(val > .5)

correlation %>% 
  ggplot( aes(var1, var2)) + # x and y axes => Var1 and Var2
  geom_tile(aes(fill = val)) + # background colours are mapped according to the value column
  # geom_text(aesX(fill = correlation$val, label = round(correlation$val, 2))) + # write the values
  scale_fill_gradient2(low = muted("darkred"), 
                       mid = "white", 
                       high = muted("midnightblue"), 
                       midpoint = 0) + # determine the colour
  theme(panel.grid.major.x=element_blank(), #no gridlines
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(),
        panel.background=element_rect(fill="white"), # background=white
        axis.text.x = element_text(angle=90, hjust = 1,vjust=1,size = 12,face = "bold"),
        plot.title = element_text(size=20,face="bold"),
        axis.text.y = element_text(size = 12,face = "bold")) + 
  ggtitle("Mapa de Calor") + 
  theme(legend.title=element_text(face="bold", size=14),
        legend.position = 'bottom') + 
  scale_x_discrete(name="") +
  scale_y_discrete(name="") +
  labs(fill="Medida de Asociación")
```


Nótese cómo sí existen departamentos cuyas probabilidades respecto al tipo de canasta son paralelas en el sentido de que se mueven en la misma direccion. Es decir, si así se deseara, se podrían agrupar los departamentos sin sufrir pérdida de información reduciéndo la dimensionalidad del problema. Esto puede verse más claro al observar el siguiente grafo de asociación.

```{r}
graficar_red_nd <- function(dat_g, subtitulo, size = 'centrality'){
  if(size == 'centrality'){
    dat_g %>% 
      ggraph(layout = 'linear', circular = TRUE) +
      geom_edge_link(aes(edge_width = val), alpha = .3,
                     arrow = arrow(length = unit(.4, 'mm')),
                     color = '#24CBE5') +
      geom_node_point(aes(size = centrality), colour = 'salmon') +
      geom_node_text(aes(label = nombre), nudge_y = 0.2, size=3, repel = TRUE) +
      theme_graph(base_family = 'sans')+
      theme(legend.position = 'bottom')+
      labs(title = 'Grafo de asociación',
           subtitle = paste(subtitulo)) 
  }else{
    dat_g %>% 
      ggraph(layout = 'linear', circular = TRUE) +
      geom_edge_link(aes(edge_width = val), alpha = .3,
                     arrow = arrow(length = unit(.4, 'mm')),
                     color = '#24CBE5') +
      geom_node_point(aes(size = importancia), colour = 'salmon') +
      geom_node_text(aes(label = nombre), nudge_y = 0.2, size=3, repel = TRUE) +
      theme_graph(base_family = 'sans')+
      theme(legend.position = 'bottom')+
      labs(title = 'Grafo de asociación',
           subtitle = paste(subtitulo)) 
  }
}

graph <- as_tbl_graph(correlation) %>%
  mutate(importancia = centrality_degree(mode = "all")) %>%
  activate(nodes) %>%
  mutate(centrality = centrality_betweenness(directed = FALSE),
         nombre = unique(correlation%>%
                           select(var1) %>%
                           rbind(correlation%>%
                                   select(var1 = var2)) %>% .$var1))


graficar_red_nd(graph, 'Agrupamiento de DepartmentType respecto al valor esperado del TripType entre canastas')

```


Esto se deja como un ejercio a futuro, por escasez de tiempo.

<!--chapter:end:02-comprension-datos.Rmd-->

# Preparación de los datos 

Este documento describe el proceso de preparación de los datos llevado a cabo sobre la información de Walmart. Cabe destacar que este comprende las etapas:

* **Selección e integración de datos**,
* **Limpieza de datos**,
* **Ingeniería de características**.

A continuación se describirá a mayor detalle cada uno de los puntos referidos. El detalle de la implementación de tales procesos se puede ver a través del archivo **DataPreparation.R**.


## 1. Selección e integración de datos

Para esta etapa se debe destacar que, como fue expuesto en la sección del análisis exploratorio de datos, la base de datos de Walmart provee información de los viajes de los clientes en una manera desagregada conforme a las visitas de tales individuos a las tiendas, en función de cada uno de los artículos que se compraron en una visita. Es decir, para cada visita de un cliente pueden existir múltiples reglones, los cuales refieren a los artículos que se adquirieron o devolvieron en dicho evento.

Desde el punto de vista del funcionamiento de los modelos de aprendizaje de máquina, esto constituye una limitante puesto que, en general, tales realizan tareas bajo la idea de que las unidades observacionales en estudio, en este caso las visitas de los clientes a la tienda, aparecen de manera única en las tablas que guardan la información correspondiente.

Es así que la primera decisión fue transformar la información a una tabla en donde se agruparan las visitas de los clientes (ver sección de ingeniería de características para mayor detalle).

## 2. Limpieza de datos

En complemento se llevaron a cabo las siguientes acciones de limpieza sobre los datos aportados por Walmart:

* Sobre los campos que contienen información tipo texto, se aplicó una transformación que convierte todo a minúsculas.
* Se realizó la imputación de valores cero sobre los campos donde existe información sobre las variables *Upc* y *FinelineNumber*.

## 3. Ingeniería de características

Sobre la base de datos en comento se creó un conjunto de nuevas variables con el propósito de nutrir con mayor elementos de información al modelo que se busca implementar. Entre tales, se encuentran:

* En primera, se identificó cuales fueron los artículos que se involucraron en la transacción de un cliente (usando el identificador de la misma),
* Posteriormente, se calculo la proporción de cada articulo que representan respecto al volumen de aquellos involucrados en la transacción (tanto de aquellos adquiridos como devueltos),
* Usando la información anterior, se consolida una nueva base de datos con la información de cada visita de un cliente a través de un renglón en donde se indica que proporción tuvo cada uno de los artículos que se vieron involucrados el evento.

Adicionalmente, se construyeron nuevas características:
  
  * A través de una nueva variable (*obj_abs*) que refleja la cantidad total de objetos que se involucraron en la transacción, donde aquellos que se adquirieron se representaron de manera agregada con signo positivo y los que se devolvieron con signo negativo.
  * La variable *num_obj* refleja el valor absoluto de la cantidad total de objetos que se involucraron en la transacción,
  * Se creó una variable indicadora(*prod_miss*) que nos dice si en la base original de Walmart se encontraba ausente el campo *Upc*.
  * Además, no se consideraron las columnas *Weekday*, *Upc* y *visitnumber*
  
Por otra parte, también se llevaron a cabo las siguientes acciones:

* Se creó una variable (*weekend*) que refleja si la transacción de un cliente se llevó a cabo o no en un fin de semana,
* En adición, se generó una variable (*day*) que codifica a manera de categorías numéricas el día de la semana (*Monday*=1, *Tuesday*=2, ..., *Sunday*=7),
* Análogamente, se construyó una variable numérica (*departmentdescription*) que codifica con categorías numéricas a los diferentes departamentos a los cuales pertenencen los artículos de la tienda.
* En lo tocante a la devolución de artículos, se crearon dos nuevas variables a) *devol* que indica si en la transacción existió un evento de devolución de artículos, y b) *porc_devol* que refleja la proporción de artículos devueltos con respecto a aquellos que se involucraron en la transacción de un cliente,


<!--chapter:end:03-preparacion-datos.Rmd-->

# Modelado

## Metodología de modelación del tipo de visitas de clientes en Walmart

En línea con lo mostrado en la etapa de análisis exploratorio, las visitas de los clientes de Walmart se asocian a combinaciones de artículos involucrados en sus transacciones. Tales se pueden abstraer a través de los departamentos en que las tiendas clasifican a su mercancia, puesto que no solo que corresponden a conjuntos de bienes de una naturaleza similar, sino que los ubican dentro de un mismo segmento del mercado en que los clientes identifican.

Ahora bien, motivados por tales hipótesis  y en razón de que para la base de datos de Walmart se decidió representar a las visitas que los clientes realizan a los establecimientos a travès de una unidad observacional que refleja en conjunto a la trasancción de artículos que hace cada cliente, en términos de la proporción de productos que comprar dentro de los departamentos, para proponer un modelo que
pueda capturar la relación entre los tipos de visitas que realizan los clientes y
dichas variables, se estimó pertinente explorar distintos modelos de aprendizaje de máquina.

La idea principal para plantearlos fue aprovechar el conocimiento de los datos de
Walmart, de manera que pudieran capturarse los efectos no lineales que se aprecian sobre las canastas de artìculos relativas a las transacciones de los clientes en cada visita. Algunos de los modelos que se conocen en la prática
que pueden reflejar dichas tendencias son aquellos basados en árboles o
ensambles de los mismos.

En tal sentido, se planteó la exploración de diversos modelos de aprendizaje de
máquina como los recién descritos, a través de una implementación de Python, en donde se calibraron sus hiper-parámetros mediante una técnica conocida como
validación cruzada.

Como resultado, se obtuvo un modelo, junto con sus correspondientes parámetros,
que tiene el mejor desempeño en la clasificación de las visitas de clientes del establecimiento referido.

Para mayor detalle, se invita al lector a consultar el archivo donde se llevó acabo la calibración de tales modelos junto con sus hiper-parámetros (*magic loop*), presente en la carpeta */build/modelado*.

<!--chapter:end:04-modelado.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

